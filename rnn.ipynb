{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# DataLoader\n",
    "################################\n",
    "\n",
    "# set up fields\n",
    "TEXT = data.Field()\n",
    "LABEL = data.Field(sequential=False, dtype=torch.long)\n",
    "\n",
    "# make splits for data\n",
    "# DO NOT MODIFY: fine_grained=True, train_subtrees=False\n",
    "train, val, test = datasets.SST.splits(\n",
    "    TEXT, LABEL, fine_grained=True, train_subtrees=False)\n",
    "\n",
    "# print information about the data\n",
    "# print('train.fields', train.fields)\n",
    "# print('len(train)', len(train))\n",
    "# print('vars(train[0])', vars(train[0]))\n",
    "\n",
    "# build the vocabulary\n",
    "# you can use other pretrained vectors, refer to https://github.com/pytorch/text/blob/master/torchtext/vocab.py\n",
    "# TEXT.build_vocab(train, vectors=Vectors(name='vector.txt', cache='.data'))\n",
    "TEXT.build_vocab(train, vectors=Vectors(name='vector.txt', cache='./data'))\n",
    "LABEL.build_vocab(train)\n",
    "# We can also see the vocabulary directly using either the stoi (string to int) or itos (int to string) method.\n",
    "# print(TEXT.vocab.itos[:10])\n",
    "# print(LABEL.vocab.stoi)\n",
    "# print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "# print vocab information\n",
    "# print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "# print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=batch_size)\n",
    "\n",
    "# print batch information\n",
    "batch = next(iter(train_iter)) # for batch in train_iter\n",
    "# print(batch.text) # input sequence\n",
    "# print(batch.label) # ground truth\n",
    "\n",
    "# Attention: batch.label in the range [1,5] not [0,4] !!!\n",
    "\n",
    "# Copy the pre-trained word embeddings we loaded earlier into the embedding layer of our model.\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "# you should maintain a nn.embedding layer in your network\n",
    "# model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# DataLoader\n",
    "################################\n",
    "\n",
    "# set up fields\n",
    "TEXT = data.Field()\n",
    "LABEL = data.Field(sequential=False,dtype=torch.long)\n",
    "\n",
    "# make splits for data\n",
    "# DO NOT MODIFY: fine_grained=True, train_subtrees=False\n",
    "train, val, test = datasets.SST.splits(\n",
    "    TEXT, LABEL, fine_grained=True, train_subtrees=False)\n",
    "\n",
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))\n",
    "\n",
    "# build the vocabulary\n",
    "# you can use other pretrained vectors, refer to https://github.com/pytorch/text/blob/master/torchtext/vocab.py\n",
    "TEXT.build_vocab(train, vectors=Vectors(name='vector.txt', cache='./data'))\n",
    "LABEL.build_vocab(train)\n",
    "# We can also see the vocabulary directly using either the stoi (string to int) or itos (int to string) method.\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(LABEL.vocab.stoi)\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=64)\n",
    "\n",
    "# print batch information\n",
    "batch = next(iter(train_iter)) # for batch in train_iter\n",
    "print(batch.text) # input sequence\n",
    "print(batch.label) # groud truth\n",
    "\n",
    "# Attention: batch.label in the range [1,5] not [0,4] !!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 5\n",
    "disp_train = 20\n",
    "disp_eval = 10\n",
    "disp_test = 10\n",
    "lmd = 0.0\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, label):\n",
    "    # out : N * 5\n",
    "    return np.count_nonzero((out.argmax(1) == label).cpu().numpy()) / out.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = 512\n",
    "hidden2 = 128\n",
    "layer = 5\n",
    "dropout = 0\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(18280, 300)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.rnn = nn.LSTM(input_size=300, hidden_size=hidden1, num_layers=layer, dropout=dropout, bidirectional=True)\n",
    "        # self.fc1 = nn.Linear(2 * hidden1, hidden2)\n",
    "        self.fc1 = nn.Linear(2 * hidden1, 5)\n",
    "        self.fc2 = nn.Linear(hidden2, 5)\n",
    "        self.acti = nn.ReLU()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = lr, weight_decay=lmd)\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(self.embedding(x))\n",
    "        y = self.fc1(out[-1, :, :])\n",
    "        # y = self.fc2(self.acti(y))\n",
    "        return y\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.train()\n",
    "        epoch_loss = 0\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            x = batch.text.to(device)\n",
    "            y = (batch.label - 1).to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self(x)\n",
    "            loss = self.criterion(output, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            if i % disp_train == 0:\n",
    "                print(f\"train batch {i}: train loss = {loss.item()}\")\n",
    "                print(f\"accu: {accuracy(output, y)}\")\n",
    "        return epoch_loss / len(train_iter)\n",
    "\n",
    "    def eval_epoch(self):\n",
    "        self.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_accu = 0\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            x = batch.text.to(device)\n",
    "            y = (batch.label - 1).to(device)\n",
    "            output = self(x)\n",
    "            loss = self.criterion(output, y)\n",
    "            epoch_loss += loss.item()\n",
    "            accu = accuracy(output, y)\n",
    "            epoch_accu += accu\n",
    "            if i % disp_eval == 0:\n",
    "                print(f\"valid batch {i}: valid loss = {loss.item()}\")\n",
    "                print(f\"accuracy: {accu}\")\n",
    "        return epoch_loss / len(val_iter), epoch_accu / len(val_iter)\n",
    "\n",
    "    def train_network(self):\n",
    "        self.to(device)\n",
    "        for e in range(num_epoch):\n",
    "            tloss = self.train_epoch()\n",
    "            vloss, accu = self.eval_epoch()\n",
    "            # self.scheduler.step()\n",
    "            print(\"-----\")\n",
    "            print(f\"epoch: {e}, average training loss: {tloss}, average validation loss: {vloss}, valid accuracy: {accu}\")\n",
    "            print(\"-----\")\n",
    "            self.train_loss.append(tloss)\n",
    "            self.accuracy.append(accu)\n",
    "            self.valid_loss.append(vloss)\n",
    "\n",
    "    def test(self):\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        test_accu = 0\n",
    "        for i, batch in enumerate(test_iter):\n",
    "            x = batch.text.to(device)\n",
    "            y = (batch.label - 1).to(device)\n",
    "            output = self(x)\n",
    "            loss = self.criterion(output, y)\n",
    "            test_loss += loss.item()\n",
    "            accu = accuracy(output, y)\n",
    "            test_accu += accu\n",
    "            if i % disp_test == 0:\n",
    "                print(f\"valid batch {i}: valid loss = {loss.item()}\")\n",
    "                print(f\"accuracy: {accu}\")\n",
    "        return test_loss / len(test_iter), test_accu / len(test_iter)\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(3,1)\n",
    "        ax[0].plot(range(num_epoch), self.train_loss)\n",
    "        ax[0].set_ylabel(\"train loss\")\n",
    "        ax[0].set_xlabel(\"epoch\")\n",
    "        ax[1].plot(range(num_epoch), self.valid_loss)\n",
    "        ax[1].set_ylabel(\"valid loss\")\n",
    "        ax[1].set_xlabel(\"epoch\")\n",
    "        ax[2].plot(range(num_epoch), self.accuracy)\n",
    "        ax[2].set_ylabel(\"accuracy\")\n",
    "        ax[2].set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModel()\n",
    "net.train_network()\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding(batch.text.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = net.embedding(batch.text.to(device))\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = net.rnn(net.embedding(batch.text.to(device)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.rnn.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0][-1,:,:]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a[1][0][-1,:,:]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a[1][0][-2,:,:]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape, c.shape, d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:, -512:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((c, d), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding(batch.text.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, a = net.test()\n",
    "print(f\"test loss: {l}, test accuracy: {a}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
